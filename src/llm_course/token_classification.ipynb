{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a5c2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ccc42cb7e249fcb21732bab60df89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36992f698c374e238e4ed34ce75068e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734f25e235ee44a0bece87dcd287a958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184e9c9dcad742da86faa05922f965b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "250f8332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU: B-ORG, 3\n",
      "rejects: O, 0\n",
      "German: B-MISC, 7\n",
      "call: O, 0\n",
      "to: O, 0\n",
      "boycott: O, 0\n",
      "British: B-MISC, 7\n",
      "lamb: O, 0\n",
      ".: O, 0\n"
     ]
    }
   ],
   "source": [
    "tokens = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "ner = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "for token, int_label in zip(tokens, ner):\n",
    "    label = ner_feature.feature.int2str(int_label)\n",
    "    print(f\"{token}: {label}, {int_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c10fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14340662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5f9420c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0bfade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aafc6930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c7f95fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]: None, -100\n",
      "EU: B-ORG, 3\n",
      "rejects: O, 0\n",
      "German: B-MISC, 7\n",
      "call: O, 0\n",
      "to: O, 0\n",
      "boycott: O, 0\n",
      "British: B-MISC, 7\n",
      "la: O, 0\n",
      "##mb: O, 0\n",
      ".: O, 0\n",
      "[SEP]: None, -100\n"
     ]
    }
   ],
   "source": [
    "tokens = inputs.tokens()\n",
    "word_ids = inputs.word_ids()\n",
    "aligned_labels = align_labels_with_tokens(labels, word_ids)\n",
    "for token, word_id, label in zip(tokens, word_ids, aligned_labels):\n",
    "    label_str = word_id if word_id is None else ner_feature.feature.int2str(label)\n",
    "    print(f\"{token}: {label_str}, {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb76a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7add7cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0d14181c0e42599d132072467b9cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65450f91e2e445695a7072f6eb4e592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41ae0bc6de44342921b08a9bc6ac571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "796fe78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46f4bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0edab208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba576ccbc1d94bc699c0bb8a40e3226b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"seqeval\", module_type: \"metric\", features: {'predictions': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence')}, usage: \"\"\"\n",
       "Produces labelling scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n",
       "    references: List of List of reference labels (Ground truth (correct) target values)\n",
       "    suffix: True if the IOB prefix is after type, False otherwise. default: False\n",
       "    scheme: Specify target tagging scheme. Should be one of [\"IOB1\", \"IOB2\", \"IOE1\", \"IOE2\", \"IOBES\", \"BILOU\"].\n",
       "        default: None\n",
       "    mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n",
       "        If you want to only count exact matches, pass mode=\"strict\". default: None.\n",
       "    sample_weight: Array-like of shape (n_samples,), weights for individual samples. default: None\n",
       "    zero_division: Which value to substitute as a metric value when encountering zero division. Should be on of 0, 1,\n",
       "        \"warn\". \"warn\" acts as 0, but the warning is raised.\n",
       "\n",
       "Returns:\n",
       "    'scores': dict. Summary of the scores for overall and per type\n",
       "        Overall:\n",
       "            'accuracy': accuracy,\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure,\n",
       "        Per type:\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> seqeval = evaluate.load(\"seqeval\")\n",
       "    >>> results = seqeval.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['MISC', 'PER', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n",
       "    >>> print(results[\"overall_f1\"])\n",
       "    0.5\n",
       "    >>> print(results[\"PER\"][\"f1\"])\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "80f6f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(raw_datasets[\"train\"].features[\"ner_tags\"].feature.names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[id2label[label] for label in label_set if label != -100] for label_set in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[pred] for (pred, label) in zip(pred_set, label_set) if label != -100]\n",
    "        for pred_set, label_set in zip(predictions, true_labels)\n",
    "    ]\n",
    "\n",
    "    #  Compute metrics\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "183fd114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3d18ad6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "84b52d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "15c2d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "620adb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='814' max='407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [407/407 06:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.213622570037842,\n",
       " 'eval_model_preparation_time': 0.0021,\n",
       " 'eval_precision': 0.004199901178795793,\n",
       " 'eval_recall': 0.04005385392123864,\n",
       " 'eval_f1': 0.007602619389873822,\n",
       " 'eval_accuracy': 0.11582386530876553,\n",
       " 'eval_runtime': 12.8528,\n",
       " 'eval_samples_per_second': 252.863,\n",
       " 'eval_steps_per_second': 31.666}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model for baseline\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "66665b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 15:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.061395</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.762362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.066799</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.764732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.062898</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.764614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nivvaknin/Desktop/LLM-course/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/nivvaknin/Desktop/LLM-course/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.06660345882381014, metrics={'train_runtime': 914.2336, 'train_samples_per_second': 46.075, 'train_steps_per_second': 5.762, 'total_flos': 920771584279074.0, 'train_loss': 0.06660345882381014, 'epoch': 3.0})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cd57b8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nivvaknin/Desktop/LLM-course/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='407' max='407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [407/407 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.06289765983819962,\n",
       " 'eval_model_preparation_time': 0.0021,\n",
       " 'eval_precision': 0.00016655562958027982,\n",
       " 'eval_recall': 0.0001682935038707506,\n",
       " 'eval_f1': 0.00016742005692281937,\n",
       " 'eval_accuracy': 0.7646141166774593,\n",
       " 'eval_runtime': 12.9851,\n",
       " 'eval_samples_per_second': 250.286,\n",
       " 'eval_steps_per_second': 31.343,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "753469ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"bert-finetuned-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "92a96779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "15f2c973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: SOCCER-JAPANGETLUCKYWIN,CHINAINSURPRISEDEFEAT.\n",
      "Tokens: ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
      "NER Tags: ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sample = raw_datasets[\"test\"][0]\n",
    "sentence = \"\".join(sample[\"tokens\"])\n",
    "tokens = sample[\"tokens\"]\n",
    "ner_tags = [id2label[label] for label in sample[\"ner_tags\"]]\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"NER Tags:\", ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "20f747c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': np.float32(0.839286),\n",
       "  'word': 'JAPANGETLUCKYWIN',\n",
       "  'start': 7,\n",
       "  'end': 23},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.64975274),\n",
       "  'word': 'CHINAIN',\n",
       "  'start': 24,\n",
       "  'end': 31}]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = ner_pipeline(sentence, aggregation_strategy=\"simple\")\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b6000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-course-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
