{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b47820",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e558e38",
   "metadata": {},
   "source": [
    "## load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d55c11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a6ed2",
   "metadata": {},
   "source": [
    "## Preprocessing (tokenization + padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e283a3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence1\"],\n",
    "        examples[\"sentence2\"],\n",
    "        truncation=True)\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832f5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45466768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([8]),\n",
       " 'input_ids': torch.Size([8, 65]),\n",
       " 'token_type_ids': torch.Size([8, 65]),\n",
       " 'attention_mask': torch.Size([8, 65])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9a257",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e311f192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a430a352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5601, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7443d",
   "metadata": {},
   "source": [
    "## Load the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f83293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b702c8a3",
   "metadata": {},
   "source": [
    "## Baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6ea7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d360c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in eval_dataloader:\n",
    "        logits = model(**batch).logits\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e620407b",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd3ee335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/mrpc-bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58926c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "DECAY = 0.01\n",
    "NUM_EPOCHS = 5\n",
    "WARMUP_STEPS = 3\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c678bac",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ef867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b71944",
   "metadata": {},
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5851a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "running_train_loss = 0.0\n",
    "running_eval_loss = 0.0\n",
    "\n",
    "for epoch in range(0, NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        running_train_loss += loss.item()\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "        metric.add_batch(predictions=preds, references=batch[\"labels\"])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Log training loss and metrics\n",
    "    avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "    avg_train_metric = metric.compute()\n",
    "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "    for k, v in avg_train_metric.items():\n",
    "        writer.add_scalar(f'Metric/train/{k}', v, epoch)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch in eval_dataloader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                running_eval_loss += loss.item()\n",
    "                preds = outputs.logits.argmax(dim=-1)\n",
    "                metric.add_batch(predictions=preds, references=batch[\"labels\"])\n",
    "            # Log evaluation loss and metrics\n",
    "            avg_eval_loss = running_eval_loss / len(eval_dataloader)\n",
    "            avg_eval_metric = metric.compute()\n",
    "            writer.add_scalar('Loss/eval', avg_eval_loss, epoch)\n",
    "            for k, v in avg_eval_metric.items():\n",
    "                writer.add_scalar(f'Metric/eval/{k}', v, epoch)\n",
    "    # Reset running losses and metrics for the next epoch\n",
    "    running_train_loss = 0.0\n",
    "    running_eval_loss = 0.0\n",
    "# Close the writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72822325",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on few examples of the test set\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "examples = raw_datasets[\"test\"][:100]\n",
    "\n",
    "success_rate = 0\n",
    "for sentence1, sentence2, lable in zip(\n",
    "    examples[\"sentence1\"], examples[\"sentence2\"], examples[\"label\"]\n",
    "):\n",
    "    print(f\"Sentence 1: {sentence1}\")\n",
    "    print(f\"Sentence 2: {sentence2}\")\n",
    "    print(f\"Label: {lable}\")\n",
    "    print(\"Prediction:\", classifier(f\"{sentence1} [SEP] {sentence2}\"))\n",
    "    print()\n",
    "    if  '1' in classifier(f\"{sentence1} [SEP] {sentence2}\")[0][\"label\"]  and 1 == lable or \\\n",
    "       '0' in classifier(f\"{sentence1} [SEP] {sentence2}\")[0][\"label\"]  and 0 == lable:\n",
    "        success_rate += 1\n",
    "\n",
    "print(f\"Success rate: {success_rate / 100 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_pretrained(\"mrpc-torch-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890365f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-course-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
